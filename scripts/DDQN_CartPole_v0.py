"""
åŒæ·±åº¦Qç½‘ç»œ(Double Deep Q-Network, DDQN)å®ç°CartPole-v0ç¯å¢ƒ
ä½œè€…: å¼ºåŒ–å­¦ä¹ åˆå­¦è€…
æ—¥æœŸ: 2025å¹´8æœˆ26æ—¥

DDQNæ˜¯DQNçš„æ”¹è¿›ç‰ˆæœ¬ï¼Œè§£å†³äº†DQNä¸­å­˜åœ¨çš„è¿‡é«˜ä¼°è®¡é—®é¢˜ï¼š
1. DQNä½¿ç”¨åŒä¸€ä¸ªç½‘ç»œæ¥é€‰æ‹©åŠ¨ä½œå’Œè¯„ä¼°åŠ¨ä½œï¼Œå®¹æ˜“å¯¼è‡´Qå€¼è¿‡é«˜ä¼°è®¡
2. DDQNä½¿ç”¨ä¸¤ä¸ªç½‘ç»œï¼šä¸»ç½‘ç»œ(policy_net)é€‰æ‹©åŠ¨ä½œï¼Œç›®æ ‡ç½‘ç»œ(target_net)è¯„ä¼°åŠ¨ä½œ
3. è¿™ç§åˆ†ç¦»ä½¿å¾—Qå€¼ä¼°è®¡æ›´åŠ å‡†ç¡®ï¼Œè®­ç»ƒæ›´ç¨³å®š
"""

import gymnasium as gym  # å¯¼å…¥Gymnasiumç¯å¢ƒåº“ï¼ˆå¼ºåŒ–å­¦ä¹ ç¯å¢ƒçš„æ ‡å‡†åº“ï¼‰
import numpy as np       # å¯¼å…¥NumPyæ•°å€¼è®¡ç®—åº“
import torch            # å¯¼å…¥PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
import torch.nn as nn   # å¯¼å…¥ç¥ç»ç½‘ç»œæ¨¡å—
import torch.optim as optim  # å¯¼å…¥ä¼˜åŒ–å™¨æ¨¡å—
import torch.nn.functional as F  # å¯¼å…¥å‡½æ•°å¼API
import matplotlib.pyplot as plt  # å¯¼å…¥matplotlibç»˜å›¾åº“ç”¨äºå¯è§†åŒ–
import matplotlib
import warnings
from collections import namedtuple, deque  # å¯¼å…¥æ•°æ®ç»“æ„
import random

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤ºå’Œå¿½ç•¥è­¦å‘Š
matplotlib.rcParams['font.sans-serif'] = ['SimHei']
matplotlib.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')

# ============================
# 1. é…ç½®å‚æ•°ç±»
# ============================
class Config:
    """
    é…ç½®ç±»ï¼šåŒ…å«æ‰€æœ‰è¶…å‚æ•°è®¾ç½®
    è¿™äº›å‚æ•°å¯¹è®­ç»ƒæ•ˆæœæœ‰é‡è¦å½±å“ï¼Œåˆå­¦è€…å¯ä»¥é€šè¿‡è°ƒæ•´è¿™äº›å‚æ•°æ¥è§‚å¯Ÿæ•ˆæœ
    """
    def __init__(self):
        # ç¯å¢ƒå‚æ•°
        self.env_name = 'CartPole-v0'  # ç¯å¢ƒåç§°ï¼šå€’ç«‹æ‘†é—®é¢˜
        
        # è®­ç»ƒå‚æ•°
        self.max_episodes = 500       # æœ€å¤§è®­ç»ƒå›åˆæ•°
        self.max_steps = 500         # æ¯å›åˆæœ€å¤§æ­¥æ•°
        
        # ç½‘ç»œå‚æ•°
        self.hidden_dim = 128        # ç¥ç»ç½‘ç»œéšè—å±‚ç»´åº¦
        self.lr = 0.001             # å­¦ä¹ ç‡ï¼šæ§åˆ¶ç½‘ç»œæƒé‡æ›´æ–°é€Ÿåº¦
        
        # DQNç‰¹æœ‰å‚æ•°
        self.batch_size = 32        # æ‰¹æ¬¡å¤§å°ï¼šæ¯æ¬¡ä»ç»éªŒå›æ”¾ä¸­é‡‡æ ·çš„ç»éªŒæ•°é‡
        self.gamma = 0.99           # æŠ˜æ‰£å› å­ï¼šæœªæ¥å¥–åŠ±çš„é‡è¦æ€§æƒé‡(0-1)
        self.epsilon_start = 1.0    # Îµ-è´ªå©ªç­–ç•¥èµ·å§‹å€¼ï¼šåˆæœŸå®Œå…¨éšæœºæ¢ç´¢
        self.epsilon_end = 0.01     # Îµ-è´ªå©ªç­–ç•¥æœ€ç»ˆå€¼ï¼šåæœŸä¸»è¦åˆ©ç”¨å·²å­¦çŸ¥è¯†
        self.epsilon_decay = 0.995  # Îµè¡°å‡ç‡ï¼šæ§åˆ¶ä»æ¢ç´¢åˆ°åˆ©ç”¨çš„è½¬æ¢é€Ÿåº¦
        
        # DDQNç‰¹æœ‰å‚æ•°
        self.target_update = 10     # ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡ï¼šæ¯10ä¸ªå›åˆæ›´æ–°ä¸€æ¬¡ç›®æ ‡ç½‘ç»œ
        self.memory_size = 10000    # ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°
        
        # è®¾å¤‡é€‰æ‹©
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ============================
# 2. ç»éªŒå›æ”¾ç¼“å†²åŒº
# ============================
# å®šä¹‰ç»éªŒå…ƒç»„ï¼šå­˜å‚¨ä¸€æ¬¡äº¤äº’çš„æ‰€æœ‰ä¿¡æ¯
Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))

class ReplayBuffer:
    """
    ç»éªŒå›æ”¾ç¼“å†²åŒºï¼šå­˜å‚¨æ™ºèƒ½ä½“ä¸ç¯å¢ƒçš„äº¤äº’ç»éªŒ
    
    ä¸ºä»€ä¹ˆéœ€è¦ç»éªŒå›æ”¾ï¼Ÿ
    1. æ‰“ç ´æ•°æ®ç›¸å…³æ€§ï¼šè¿ç»­çš„ç»éªŒå¾€å¾€é«˜åº¦ç›¸å…³ï¼Œç›´æ¥ç”¨äºè®­ç»ƒä¼šå¯¼è‡´ç½‘ç»œè¿‡æ‹Ÿåˆ
    2. æé«˜æ ·æœ¬åˆ©ç”¨æ•ˆç‡ï¼šä¸€ä¸ªç»éªŒå¯ä»¥è¢«å¤šæ¬¡ä½¿ç”¨ï¼Œæé«˜æ•°æ®åˆ©ç”¨ç‡
    3. ç¨³å®šè®­ç»ƒï¼šéšæœºé‡‡æ ·ç»éªŒä½¿å¾—è®­ç»ƒæ›´åŠ ç¨³å®š
    """
    def __init__(self, capacity):
        """
        åˆå§‹åŒ–ç»éªŒç¼“å†²åŒº
        Args:
            capacity: ç¼“å†²åŒºæœ€å¤§å®¹é‡
        """
        self.buffer = deque(maxlen=capacity)  # ä½¿ç”¨åŒç«¯é˜Ÿåˆ—ï¼Œè‡ªåŠ¨ä¿æŒå›ºå®šå¤§å°
    
    def push(self, state, action, reward, next_state, done):
        """
        å­˜å‚¨ä¸€æ¬¡ç»éªŒ
        Args:
            state: å½“å‰çŠ¶æ€
            action: æ‰§è¡Œçš„åŠ¨ä½œ
            reward: è·å¾—çš„å¥–åŠ±
            next_state: ä¸‹ä¸€ä¸ªçŠ¶æ€
            done: æ˜¯å¦ç»“æŸ
        """
        # å°†numpyæ•°ç»„è½¬æ¢ä¸ºtensorå¹¶æ·»åŠ åˆ°ç¼“å†²åŒº
        state = torch.FloatTensor(state).unsqueeze(0) if isinstance(state, np.ndarray) else state
        next_state = torch.FloatTensor(next_state).unsqueeze(0) if isinstance(next_state, np.ndarray) else next_state
        
        self.buffer.append(Transition(state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        """
        ä»ç¼“å†²åŒºéšæœºé‡‡æ ·ä¸€æ‰¹ç»éªŒ
        Args:
            batch_size: é‡‡æ ·æ•°é‡
        Returns:
            æ‰¹æ¬¡ç»éªŒæ•°æ®
        """
        return random.sample(self.buffer, batch_size)
    
    def __len__(self):
        """è¿”å›å½“å‰ç¼“å†²åŒºå¤§å°"""
        return len(self.buffer)

# ============================
# 3. æ·±åº¦Qç½‘ç»œç»“æ„
# ============================
class DQN(nn.Module):
    """
    æ·±åº¦Qç½‘ç»œï¼šç”¨ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼Qå‡½æ•°
    
    Qå‡½æ•°Q(s,a)è¡¨ç¤ºåœ¨çŠ¶æ€sä¸‹æ‰§è¡ŒåŠ¨ä½œaçš„æœŸæœ›ç´¯ç§¯å¥–åŠ±
    ç¥ç»ç½‘ç»œè¾“å…¥çŠ¶æ€sï¼Œè¾“å‡ºæ¯ä¸ªåŠ¨ä½œçš„Qå€¼
    """
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        """
        åˆå§‹åŒ–ç½‘ç»œç»“æ„
        Args:
            state_dim: çŠ¶æ€ç»´åº¦ï¼ˆCartPole-v0ä¸­ä¸º4ç»´ï¼šä½ç½®ã€é€Ÿåº¦ã€è§’åº¦ã€è§’é€Ÿåº¦ï¼‰
            action_dim: åŠ¨ä½œç»´åº¦ï¼ˆCartPole-v0ä¸­ä¸º2ç»´ï¼šå·¦æ¨ã€å³æ¨ï¼‰ è¿™é‡Œè¾“å‡ºçš„Q(s,a)
            hidden_dim: éšè—å±‚ç»´åº¦
        """
        super(DQN, self).__init__()
        
        # å®šä¹‰å…¨è¿æ¥ç¥ç»ç½‘ç»œ
        self.fc1 = nn.Linear(state_dim, hidden_dim)    # è¾“å…¥å±‚åˆ°éšè—å±‚1
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)   # éšè—å±‚1åˆ°éšè—å±‚2
        self.fc3 = nn.Linear(hidden_dim, action_dim)   # éšè—å±‚2åˆ°è¾“å‡ºå±‚
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ï¼šè®¡ç®—Qå€¼
        Args:
            x: è¾“å…¥çŠ¶æ€
        Returns:
            æ¯ä¸ªåŠ¨ä½œå¯¹åº”çš„Qå€¼
        """
        x = F.relu(self.fc1(x))  # ç¬¬ä¸€å±‚ + ReLUæ¿€æ´»å‡½æ•°
        x = F.relu(self.fc2(x))  # ç¬¬äºŒå±‚ + ReLUæ¿€æ´»å‡½æ•°
        x = self.fc3(x)          # è¾“å‡ºå±‚ï¼ˆä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°ï¼Œå› ä¸ºQå€¼å¯ä»¥æ˜¯ä»»æ„å®æ•°ï¼‰
        return x

# ============================
# 4. DDQNæ™ºèƒ½ä½“
# ============================
class DDQNAgent:
    """
    åŒæ·±åº¦Qç½‘ç»œæ™ºèƒ½ä½“ï¼šå®ç°DDQNç®—æ³•çš„æ ¸å¿ƒç±»
    
    DDQNç›¸æ¯”DQNçš„æ”¹è¿›ï¼š
    1. ä½¿ç”¨ä¸»ç½‘ç»œé€‰æ‹©åŠ¨ä½œï¼ša* = argmax Q_main(s', a)
    2. ä½¿ç”¨ç›®æ ‡ç½‘ç»œè¯„ä¼°åŠ¨ä½œä»·å€¼ï¼šQ_target(s', a*)
    3. è¿™æ ·é¿å…äº†åŒä¸€ç½‘ç»œæ—¢é€‰æ‹©åˆè¯„ä¼°å¯¼è‡´çš„è¿‡é«˜ä¼°è®¡é—®é¢˜
    """
    def __init__(self, state_dim, action_dim, cfg):
        """
        åˆå§‹åŒ–DDQNæ™ºèƒ½ä½“
        Args:
            state_dim: çŠ¶æ€ç»´åº¦
            action_dim: åŠ¨ä½œç»´åº¦  
            cfg: é…ç½®å‚æ•°
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.cfg = cfg
        
        # åˆ›å»ºä¸¤ä¸ªç›¸åŒç»“æ„çš„ç¥ç»ç½‘ç»œ
        self.policy_net = DQN(state_dim, action_dim, cfg.hidden_dim).to(cfg.device)  # ä¸»ç½‘ç»œï¼šç”¨äºé€‰æ‹©åŠ¨ä½œ
        self.target_net = DQN(state_dim, action_dim, cfg.hidden_dim).to(cfg.device)  # ç›®æ ‡ç½‘ç»œï¼šç”¨äºè®¡ç®—ç›®æ ‡Qå€¼
        
        # åˆå§‹åŒ–æ—¶ç›®æ ‡ç½‘ç»œæƒé‡ä¸ä¸»ç½‘ç»œç›¸åŒ
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()  # ç›®æ ‡ç½‘ç»œè®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œä¸å‚ä¸æ¢¯åº¦æ›´æ–°
        
        # ä¼˜åŒ–å™¨ï¼šç”¨äºæ›´æ–°ä¸»ç½‘ç»œæƒé‡
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=cfg.lr)
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.memory = ReplayBuffer(cfg.memory_size)
        
        # Îµ-è´ªå©ªç­–ç•¥å‚æ•°
        self.epsilon = cfg.epsilon_start
        
        # è®°å½•è®­ç»ƒæ­¥æ•°
        self.steps_done = 0
    
    def select_action(self, state):
        """
        ä½¿ç”¨Îµ-è´ªå©ªç­–ç•¥é€‰æ‹©åŠ¨ä½œ
        
        Îµ-è´ªå©ªç­–ç•¥ï¼š
        - ä»¥Îµçš„æ¦‚ç‡éšæœºé€‰æ‹©åŠ¨ä½œï¼ˆæ¢ç´¢ï¼Œexplorationï¼‰
        - ä»¥1-Îµçš„æ¦‚ç‡é€‰æ‹©å½“å‰è®¤ä¸ºæœ€ä¼˜çš„åŠ¨ä½œï¼ˆåˆ©ç”¨ï¼Œexploitationï¼‰
        - Îµä¼šéšç€è®­ç»ƒé€æ¸å‡å°ï¼Œä»æ¢ç´¢è½¬å‘åˆ©ç”¨
        
        Args:
            state: å½“å‰çŠ¶æ€
        Returns:
            é€‰æ‹©çš„åŠ¨ä½œ
        """
        if random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©åŠ¨ä½œ
            return random.randrange(self.action_dim)
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ
            with torch.no_grad():  # ä¸éœ€è¦è®¡ç®—æ¢¯åº¦ï¼ŒèŠ‚çœå†…å­˜å’Œè®¡ç®—
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.cfg.device)
                q_values = self.policy_net(state_tensor)  # è®¡ç®—æ‰€æœ‰åŠ¨ä½œçš„Qå€¼
                return q_values.argmax().item()  # è¿”å›Qå€¼æœ€å¤§çš„åŠ¨ä½œç´¢å¼•
    
    def update(self):
        """
        æ›´æ–°ç½‘ç»œæƒé‡ï¼šDDQNç®—æ³•çš„æ ¸å¿ƒ
        
        DDQNæ›´æ–°è¿‡ç¨‹ï¼š
        1. ä»ç»éªŒç¼“å†²åŒºé‡‡æ ·ä¸€æ‰¹ç»éªŒ
        2. ä½¿ç”¨ä¸»ç½‘ç»œåœ¨ä¸‹ä¸€çŠ¶æ€é€‰æ‹©åŠ¨ä½œ
        3. ä½¿ç”¨ç›®æ ‡ç½‘ç»œè¯„ä¼°è¯¥åŠ¨ä½œçš„ä»·å€¼
        4. è®¡ç®—ç›®æ ‡Qå€¼å’Œå½“å‰Qå€¼çš„å·®å¼‚ï¼ˆTDè¯¯å·®ï¼‰
        5. ä½¿ç”¨æ¢¯åº¦ä¸‹é™æœ€å°åŒ–TDè¯¯å·®
        """
        # å¦‚æœç»éªŒä¸è¶³ï¼Œä¸è¿›è¡Œæ›´æ–°
        if len(self.memory) < self.cfg.batch_size:
            return
        
        # ä»ç»éªŒç¼“å†²åŒºé‡‡æ ·
        transitions = self.memory.sample(self.cfg.batch_size)
        batch = Transition(*zip(*transitions))
        
        # å°†æ•°æ®è½¬æ¢ä¸ºtensor
        state_batch = torch.cat(batch.state).to(self.cfg.device)
        action_batch = torch.LongTensor(batch.action).to(self.cfg.device)
        reward_batch = torch.FloatTensor(batch.reward).to(self.cfg.device)
        next_state_batch = torch.cat(batch.next_state).to(self.cfg.device)
        done_batch = torch.BoolTensor(batch.done).to(self.cfg.device)
        
        # è´å°”æ›¼æ–¹ç¨‹çš„æ•°å­¦è¡¨è¾¾ï¼š
        # Q(s,a) = r + Î³ * max_a' Q(s',a')
        # ç¿»è¯‘æˆè‡ªç„¶è¯­è¨€ï¼š
        # "åœ¨çŠ¶æ€sæ‰§è¡ŒåŠ¨ä½œaçš„ä»·å€¼" = "ç«‹å³å¥–åŠ±r" + "æŠ˜æ‰£åçš„ä¸‹ä¸€çŠ¶æ€æœ€å¤§ä»·å€¼"
        # è®¡ç®—å½“å‰çŠ¶æ€çš„Qå€¼ï¼šQ(s_t, a_t) éœ€è¦æ›´æ–°çš„ç›®æ ‡
        current_q_values = self.policy_net(state_batch).gather(1, action_batch.unsqueeze(1))
        
        # DDQNçš„æ ¸å¿ƒï¼šåˆ†åˆ«ä½¿ç”¨ä¸»ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        with torch.no_grad():
            # ä½¿ç”¨ä¸»ç½‘ç»œé€‰æ‹©ä¸‹ä¸€çŠ¶æ€çš„æœ€ä¼˜åŠ¨ä½œ
            next_actions = self.policy_net(next_state_batch).argmax(1)
            # ä½¿ç”¨ç›®æ ‡ç½‘ç»œè¯„ä¼°é€‰å®šåŠ¨ä½œçš„Qå€¼, å»¶è¿Ÿæ›´æ–°çš„æ—§çš„Qå€¼
            next_q_values = self.target_net(next_state_batch).gather(1, next_actions.unsqueeze(1)).squeeze(1)
            # è´å°”æ›¼æ–¹ç¨‹è®¡ç®—ç›®æ ‡Qå€¼ï¼šr + Î³ * max_a Q_target(s', a) ï¼ˆå¦‚æœæœªç»“æŸï¼‰
            # æ•°å­¦ä¸Šå¯ä»¥è¯æ˜ä¸€å®šæ¡ä»¶ä¸‹è¿™ä¸ªè¿‡ç¨‹ä¼šæ”¶æ•›åˆ°Q*
            # å…³é”®ç†è§£ï¼šä¸ºä»€ä¹ˆè¦ä¹˜ä»¥ ~done_batch
            # è®©æˆ‘ä»¬åˆ†æƒ…å†µè¯¦ç»†è§£é‡Šï¼š
            # æƒ…å†µ1ï¼šå¦‚æœå›åˆæœªç»“æŸ (done=False, ~done=True)
            # Q(s,a) = r + Î³ * max_a' Q(s',a')  # æ ‡å‡†çš„è´å°”æ›¼æ–¹ç¨‹
            # æƒ…å†µ2ï¼šå¦‚æœå›åˆå·²ç»“æŸ (done=True, ~done=False)  
            # Q(s,a) = r + Î³ * 0 = r  # å› ä¸ºç»ˆæ­¢çŠ¶æ€æ²¡æœ‰ä¸‹ä¸€ä¸ªçŠ¶æ€ï¼Œæœªæ¥å¥–åŠ±ä¸º0
            target_q_values = reward_batch + (self.cfg.gamma * next_q_values * ~done_batch)
            
        # è®¡ç®—æŸå¤±ï¼šå‡æ–¹è¯¯å·®
        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)
        
        # åå‘ä¼ æ’­æ›´æ–°ç½‘ç»œæƒé‡
        self.optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦
        loss.backward()             # è®¡ç®—æ¢¯åº¦
        
        # æ¢¯åº¦è£å‰ªï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), max_norm=10)
        
        self.optimizer.step()       # æ›´æ–°æƒé‡
        
        # æ›´æ–°Îµå€¼ï¼šé€æ¸å‡å°‘éšæœºæ¢ç´¢
        self.epsilon = max(self.cfg.epsilon_end, self.epsilon * self.cfg.epsilon_decay)

# ============================
# 5. è®­ç»ƒå‡½æ•°
# ============================
def train_ddqn():
    """
    è®­ç»ƒDDQNæ™ºèƒ½ä½“çš„ä¸»å‡½æ•°
    """
    # åˆå§‹åŒ–é…ç½®
    cfg = Config()
    
    # åˆ›å»ºç¯å¢ƒ
    env = gym.make(cfg.env_name)
    env.action_space.seed(1)    # è®¾ç½®åŠ¨ä½œç©ºé—´éšæœºç§å­
    
    # è·å–çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ç»´åº¦
    state_dim = env.observation_space.shape[0]  # çŠ¶æ€ç»´åº¦ï¼š4ï¼ˆä½ç½®ã€é€Ÿåº¦ã€è§’åº¦ã€è§’é€Ÿåº¦ï¼‰
    action_dim = env.action_space.n             # åŠ¨ä½œç»´åº¦ï¼š2ï¼ˆå·¦æ¨ã€å³æ¨ï¼‰
    
    print(f"ç¯å¢ƒä¿¡æ¯ï¼š")
    print(f"- çŠ¶æ€ç»´åº¦: {state_dim}")
    print(f"- åŠ¨ä½œç»´åº¦: {action_dim}")
    print(f"- è®¾å¤‡: {cfg.device}")
    print("-" * 50)
    
    # åˆ›å»ºDDQNæ™ºèƒ½ä½“
    agent = DDQNAgent(state_dim, action_dim, cfg)
    
    # è®°å½•è®­ç»ƒè¿‡ç¨‹çš„æŒ‡æ ‡
    rewards = []                    # æ¯å›åˆçš„æ€»å¥–åŠ±
    moving_average_rewards = []     # æ»‘åŠ¨å¹³å‡å¥–åŠ±ï¼ˆå¹³æ»‘æ›²çº¿ï¼‰
    ep_steps = []                   # æ¯å›åˆçš„æ­¥æ•°
    
    # å¼€å§‹è®­ç»ƒ
    for i_episode in range(1, cfg.max_episodes + 1):
        # é‡ç½®ç¯å¢ƒï¼Œè·å–åˆå§‹çŠ¶æ€
        state, _ = env.reset()
        ep_reward = 0  # å½“å‰å›åˆç´¯ç§¯å¥–åŠ±
        
        # åœ¨å½“å‰å›åˆä¸­ä¸ç¯å¢ƒäº¤äº’
        for i_step in range(1, cfg.max_steps + 1):
            # æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œ
            action = agent.select_action(state)
            
            # æ‰§è¡ŒåŠ¨ä½œï¼Œè·å–ç¯å¢ƒåé¦ˆ
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated  # å›åˆæ˜¯å¦ç»“æŸ
            
            # ç´¯ç§¯å¥–åŠ±
            ep_reward += reward
            
            # å°†ç»éªŒå­˜å…¥å›æ”¾ç¼“å†²åŒº
            agent.memory.push(state, action, reward, next_state, done)
            
            # çŠ¶æ€è½¬ç§»
            state = next_state
            
            # æ›´æ–°ç½‘ç»œæƒé‡
            agent.update()
            
            # å¦‚æœå›åˆç»“æŸï¼Œè·³å‡ºå¾ªç¯
            if done:
                break
        
        # å®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œï¼šå°†ä¸»ç½‘ç»œçš„æƒé‡å¤åˆ¶åˆ°ç›®æ ‡ç½‘ç»œ
        if i_episode % cfg.target_update == 0:
            agent.target_net.load_state_dict(agent.policy_net.state_dict())
            print(f"å›åˆ {i_episode}: ç›®æ ‡ç½‘ç»œå·²æ›´æ–°")
        
        # è®°å½•è®­ç»ƒæŒ‡æ ‡
        ep_steps.append(i_step)
        rewards.append(ep_reward)
        
        # è®¡ç®—æ»‘åŠ¨å¹³å‡å¥–åŠ±ï¼ˆç”¨äºå¹³æ»‘æ˜¾ç¤ºè®­ç»ƒæ›²çº¿ï¼‰
        if i_episode == 1:
            moving_average_rewards.append(ep_reward)
        else:
            # æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼šæ–°å€¼æƒé‡0.1ï¼Œå†å²å€¼æƒé‡0.9
            moving_average_rewards.append(
                0.9 * moving_average_rewards[-1] + 0.1 * ep_reward
            )
        
        # æ‰“å°è®­ç»ƒè¿›åº¦
        if i_episode % 10 == 0:
            avg_reward = np.mean(rewards[-10:])  # æœ€è¿‘10å›åˆå¹³å‡å¥–åŠ±
            print(f"å›åˆ {i_episode:3d} | "
                  f"å¥–åŠ±: {int(ep_reward):3d} | "
                  f"æ­¥æ•°: {i_step:3d} | "
                  f"å¹³å‡å¥–åŠ±: {avg_reward:.1f} | "
                  f"æ¢ç´¢ç‡: {agent.epsilon:.3f}")
        
        # åˆ¤æ–­æ˜¯å¦å·²ç»å­¦ä¼šï¼ˆè¿ç»­100å›åˆå¹³å‡å¥–åŠ±>=195ï¼‰
        if len(rewards) >= 100 and np.mean(rewards[-100:]) >= 195:
            print(f"\nğŸ‰ åœ¨å›åˆ {i_episode} è§£å†³äº†ç¯å¢ƒï¼")
            print(f"æœ€è¿‘100å›åˆå¹³å‡å¥–åŠ±: {np.mean(rewards[-100:]):.1f}")
            break
    
    # å…³é—­ç¯å¢ƒ
    env.close()
    
    # è¿”å›è®­ç»ƒç»“æœ
    return rewards, moving_average_rewards, ep_steps

# ============================
# 6. ç»“æœå¯è§†åŒ–
# ============================
def plot_results(rewards, moving_average_rewards, ep_steps):
    """
    ç»˜åˆ¶è®­ç»ƒç»“æœå›¾è¡¨
    """
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('DDQNè®­ç»ƒç»“æœåˆ†æ', fontsize=16)
    
    # å¥–åŠ±æ›²çº¿
    axes[0, 0].plot(rewards, alpha=0.6, color='blue', label='æ¯å›åˆå¥–åŠ±')
    axes[0, 0].plot(moving_average_rewards, color='red', linewidth=2, label='æ»‘åŠ¨å¹³å‡å¥–åŠ±')
    axes[0, 0].set_xlabel('è®­ç»ƒå›åˆ')
    axes[0, 0].set_ylabel('ç´¯ç§¯å¥–åŠ±')
    axes[0, 0].set_title('å¥–åŠ±å­¦ä¹ æ›²çº¿')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # æ­¥æ•°æ›²çº¿
    axes[0, 1].plot(ep_steps, color='green', alpha=0.7)
    axes[0, 1].set_xlabel('è®­ç»ƒå›åˆ')
    axes[0, 1].set_ylabel('æ¯å›åˆæ­¥æ•°')
    axes[0, 1].set_title('æ¯å›åˆç”Ÿå­˜æ­¥æ•°')
    axes[0, 1].grid(True, alpha=0.3)
    
    # å¥–åŠ±åˆ†å¸ƒç›´æ–¹å›¾
    axes[1, 0].hist(rewards, bins=30, alpha=0.7, color='purple', edgecolor='black')
    axes[1, 0].set_xlabel('ç´¯ç§¯å¥–åŠ±')
    axes[1, 0].set_ylabel('é¢‘æ¬¡')
    axes[1, 0].set_title('å¥–åŠ±åˆ†å¸ƒç›´æ–¹å›¾')
    axes[1, 0].grid(True, alpha=0.3)
    
    # è®­ç»ƒè¿›åº¦åˆ†æ
    if len(rewards) >= 100:
        # è®¡ç®—æ»‘åŠ¨çª—å£å¹³å‡å¥–åŠ±
        window_size = 100
        avg_rewards = []
        for i in range(window_size - 1, len(rewards)):
            avg_rewards.append(np.mean(rewards[i - window_size + 1:i + 1]))
        
        axes[1, 1].plot(range(window_size, len(rewards) + 1), avg_rewards, 
                       color='orange', linewidth=2)
        axes[1, 1].axhline(y=195, color='red', linestyle='--', 
                          label='è§£å†³é˜ˆå€¼(195)')
        axes[1, 1].set_xlabel('è®­ç»ƒå›åˆ')
        axes[1, 1].set_ylabel('100å›åˆå¹³å‡å¥–åŠ±')
        axes[1, 1].set_title('å­¦ä¹ è¿›åº¦ï¼ˆ100å›åˆæ»‘åŠ¨å¹³å‡ï¼‰')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # ä¿å­˜å›¾ç‰‡
    plt.savefig('ddqn_training_results.png', dpi=300, bbox_inches='tight')
    print("è®­ç»ƒç»“æœå›¾è¡¨å·²ä¿å­˜ä¸º 'ddqn_training_results.png'")

# ============================
# 7. ä¸»å‡½æ•°
# ============================
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ åŒæ·±åº¦Qç½‘ç»œ(DDQN) - CartPole-v0 è®­ç»ƒå¼€å§‹")
    print("=" * 60)
    
    # å¼€å§‹è®­ç»ƒ
    rewards, moving_average_rewards, ep_steps = train_ddqn()
    
    print("\n" + "=" * 60)
    print("ğŸ“Š è®­ç»ƒå®Œæˆï¼Œå¼€å§‹åˆ†æç»“æœ...")
    print("=" * 60)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“ˆ è®­ç»ƒç»Ÿè®¡ä¿¡æ¯ï¼š")
    print(f"- æ€»å›åˆæ•°: {len(rewards)}")
    print(f"- æœ€é«˜å•å›åˆå¥–åŠ±: {max(rewards)}")
    print(f"- å¹³å‡å¥–åŠ±: {np.mean(rewards):.2f}")
    print(f"- æœ€å100å›åˆå¹³å‡å¥–åŠ±: {np.mean(rewards[-100:]):.2f}")
    
    # ç»˜åˆ¶ç»“æœ
    plot_results(rewards, moving_average_rewards, ep_steps)
